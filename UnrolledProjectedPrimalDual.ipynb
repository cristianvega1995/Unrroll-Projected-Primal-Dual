{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/cristianvega1995/Unrroll-Projected-Primal-Dual/blob/main/UnrolledProjectedPrimalDual.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "9OYjpAC7POeX"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import pywt\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import torch.optim as optim\n",
        "import time\n",
        "import matplotlib.pyplot as plt\n",
        "import os\n",
        "import torch.nn.functional as FF\n",
        "import shutil  # For deleting the folder\n",
        "import scipy\n",
        "import tensorflow as tf\n",
        "import matplotlib.cm as cm\n",
        "import scipy.ndimage\n",
        "\n",
        "from torch.autograd import Variable\n",
        "from torch.utils.data import DataLoader\n",
        "from sklearn.model_selection import train_test_split\n",
        "from skimage import io\n",
        "from skimage import metrics\n",
        "from skimage.color import rgb2gray\n",
        "from skimage.transform import resize\n",
        "\n",
        "device = (\"cuda\" if torch.cuda.is_available() else \"cpu\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "1VOr9bO0Gwd-"
      },
      "outputs": [],
      "source": [
        "from google.colab import drive\n",
        "\n",
        "# Mount Google Drive\n",
        "drive.mount('/content/drive')\n",
        "\n",
        "# Define the path where you want to copy the Dataset1 folder in Colab\n",
        "colab_folder = \"/content/Dataset1\"\n",
        "\n",
        "# Copy the Dataset1 folder from Google Drive to Colab\n",
        "!cp -r \"/content/drive/MyDrive/Dataset1\" \"$colab_folder\"\n",
        "\n",
        "# Check if the copy was successful\n",
        "!ls \"$colab_folder\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "1vmCVmGzfrw1"
      },
      "outputs": [],
      "source": [
        "\"\"\"\n",
        "module.\n",
        "Class\n",
        "-----------\n",
        "MyDataset : manages loading data to be fed to the first layer of unrolled architecture.\n",
        "\"\"\"\n",
        "\n",
        "class MyDataset(torch.utils.data.Dataset):\n",
        "    def __init__(self, folder,initial_x0, need_names):\n",
        "        super(MyDataset, self).__init__()\n",
        "        self.folder_Gr = os.path.join(folder, \"Groundtruth\")\n",
        "        self.folder_Dr = os.path.join(folder, \"Degraded\")\n",
        "\n",
        "        self.file_names_Gr = os.listdir(self.folder_Gr)\n",
        "        self.file_names_Dr = os.listdir(self.folder_Dr)\n",
        "        if self.file_names_Gr.count('.ipynb_checkpoints') !=0:\n",
        "            self.file_names_Gr.remove('.ipynb_checkpoints')\n",
        "\n",
        "        self.file_list_Gr = [os.path.join(self.folder_Gr, i) for i in self.file_names_Gr if not i.startswith('.')]\n",
        "        self.file_list_Dr = [os.path.join(self.folder_Dr, i) for i in self.file_names_Dr if not i.startswith('.')]\n",
        "        self.need_names = need_names\n",
        "        self.initial_x0=initial_x0\n",
        "\n",
        "    def __getitem__(self, index):\n",
        "        #print(index)\n",
        "        index = index%2000\n",
        "        X_true = np.load(self.file_list_Gr[index], allow_pickle=True)\n",
        "        Degraded_path = (self.file_list_Gr[index].replace('Groundtruth', 'Degraded')).replace('Gr_', 'Dr_')\n",
        "        X_degraded = np.load(Degraded_path, allow_pickle=True)\n",
        "        if self.need_names == 'no':\n",
        "            return X_true, X_degraded\n",
        "        if self.need_names == \"yes\":\n",
        "            name=os.path.splitext(self.file_names_Gr[index])[0]\n",
        "            return name, X_true, X_degraded\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.file_list_Gr)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "hot0YskzP__c"
      },
      "outputs": [],
      "source": [
        "def soft_thresh(w, mu):\n",
        "    w -= np.clip(w, -mu, mu)\n",
        "    return w\n",
        "\n",
        "\n",
        "def Loss_fun(A, B):\n",
        "    return F.mse_loss(A, B)\n",
        "\n",
        "#def Loss_fun(A, B):\n",
        "#    return nn.MSELoss(A,B)\n",
        "\n",
        "def activation_primal(u, tau, factor=1.): #soft- thresh in torch\n",
        "    zer = torch.zeros_like(u)\n",
        "    return torch.sign(u) * torch.maximum(zer, torch.abs(u) - tau)\n",
        "def activation_primal_wavelet(u, tau, factor=1.):\n",
        "    # Move tensor to CPU\n",
        "    u_cpu = u.cpu()\n",
        "\n",
        "    # Apply Daubechies wavelet transformation\n",
        "    wavelet = 'db4'  # You can choose the appropriate Daubechies wavelet\n",
        "    coeffs = pywt.wavedec(u_cpu.detach().numpy(), wavelet)\n",
        "\n",
        "    # Thresholding (soft-thresholding in this case)\n",
        "    threshold = tau.item() * factor\n",
        "    coeffs_thresholded = [pywt.threshold(c, threshold, mode='soft') for c in coeffs]\n",
        "\n",
        "    # Inverse wavelet transform\n",
        "    u_wavelet = torch.from_numpy(pywt.waverec(coeffs_thresholded, wavelet)).to(u.device)\n",
        "\n",
        "    return u_wavelet"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "vdCHLz3SiBAh"
      },
      "outputs": [],
      "source": [
        "# Unrolling\n",
        "\n",
        "# Create the stepsize architecture in pytorch\n",
        "\n",
        "R = nn.ReLU()\n",
        "class stepsize_arch(torch.nn.Module): # Architecture of learnable parameters\n",
        "    def __init__(self):\n",
        "        super(stepsize_arch, self).__init__()\n",
        "        self.tau = nn.Parameter(torch.DoubleTensor([10]).cuda(), requires_grad=True)\n",
        "        self.sigma = nn.Parameter(torch.DoubleTensor([0.01]).cuda(), requires_grad=True)\n",
        "        self.rho = nn.Parameter(torch.DoubleTensor([1]).cuda(), requires_grad=True)\n",
        "        # 10, 0,01 and 1 are the initializations of the parameters\n",
        "\n",
        "    def forward(self):\n",
        "        tau = R(self.tau)\n",
        "        sigma = R(self.sigma)\n",
        "        rho = R(self.rho) # Non-negativity constraints for all of the parameters\n",
        "        return (tau, sigma, rho)\n",
        "\n",
        "class layer(torch.nn.Module):\n",
        "    def __init__(self):\n",
        "        super(layer, self).__init__()\n",
        "        self.architecture = stepsize_arch()\n",
        "\n",
        "    def forward(self, H, p, p_old, d, d_old, x, x_old, y): # This defines one layer of the neural architecture\n",
        "        tau, sigma, rho = self.architecture()\n",
        "        x = x.to(device)\n",
        "        x_old = x_old.to(device)\n",
        "        p = p.to(device)\n",
        "        p_old = p_old.to(device)\n",
        "        H = H.to(device)\n",
        "        y = y.to(device)\n",
        "        d_old = d\n",
        "        d = d + sigma * (torch.t(torch.mm(H, torch.t(x + p - p_old))) - y)\n",
        "        bd = - tau * torch.t(torch.mm(torch.t(H), torch.t(d)))\n",
        "        x_old = x\n",
        "        x = activation_primal(p + bd, 0.01*tau)\n",
        "        bp = torch.t(torch.mm(H, torch.t(x))) - y\n",
        "        bp = torch.t(torch.mm(torch.t(H), torch.t(bp)))\n",
        "        p_old = p\n",
        "        p = x - rho * bp\n",
        "        return p, p_old, d, d_old, x, x_old\n",
        "\n",
        "\n",
        "class PD_model(torch.nn.Module):\n",
        "    def __init__(self, num_layers, H):\n",
        "        super(PD_model, self).__init__()\n",
        "        self.Layers = nn.ModuleList()\n",
        "        for i in range(num_layers):\n",
        "            self.Layers.append(layer())\n",
        "\n",
        "    def forward(self, H, p0, p0_old, d0, d0_old, x0, x0_old, y, x_true): # This is the whole NN, with many layers as num_layers\n",
        "        for i, l in enumerate(self.Layers):\n",
        "            # mse = Loss_fun(p0, x_true)\n",
        "            p_update, p_old_update, d_update, d_old_update, x_update, x_old_update = self.Layers[i](H, p0, p0_old, d0, d0_old, x0, x0_old, y)\n",
        "            p0 = p_update\n",
        "            p0_old = p_old_update\n",
        "            x0 = x_update\n",
        "            x0_old = x_old_update\n",
        "            d0 = d_update\n",
        "            d0_old = d_old_update\n",
        "        return p_update, d_update\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "9wNcS0zBBIwQ"
      },
      "outputs": [],
      "source": [
        "class Network(nn.Module):\n",
        "    def __init__(self, Initialization, train_conditions, paths):\n",
        "        super(Network,self).__init__()\n",
        "        self.number_layers, self.H, self.initial_x0 = Initialization\n",
        "        self.number_epochs, self.lr, self.train_batch_size, self.val_batch_size, self.test_batch_size = train_conditions\n",
        "        self.path_train, self.path_validation, self.path_test, self.path_save_model = paths\n",
        "        self.model = PD_model(self.number_layers, self.H).cuda()\n",
        "        self.dtype = torch.cuda.DoubleTensor\n",
        "\n",
        "    def CreateLoader(self, need_names): # data loader\n",
        "        train_data = MyDataset(self.path_train, self.initial_x0, need_names)\n",
        "        self.train_loader = DataLoader(train_data, batch_size=self.train_batch_size, shuffle=True)\n",
        "        val_data = MyDataset(self.path_validation,self.initial_x0, need_names)\n",
        "        self.val_loader = DataLoader(val_data, batch_size=self.val_batch_size, shuffle=True)\n",
        "        test_data = MyDataset(self.path_test, self.initial_x0, need_names)\n",
        "        self.test_loader = DataLoader(test_data, batch_size=self.test_batch_size, shuffle=True)\n",
        "        #print(self.test_loader)\n",
        "########################################################\n",
        "\n",
        "    def train(self, number_try, need_names, path_model=None):\n",
        "        epoc = 0\n",
        "        optimizer = optim.Adam(self.model.parameters(), lr=self.lr)\n",
        "\n",
        "        if not os.path.exists(os.path.join(self.path_save_model,'Trained_Model'+'_'+str(number_try))):\n",
        "            os.makedirs(os.path.join(self.path_save_model,'Trained_Model'+'_'+str(number_try)))\n",
        "            file_object = open(os.path.join(self.path_save_model,'Trained_Model'+'_'+str(number_try)) + \"/readme.txt\", \"a\")\n",
        "            file_object.writelines([    \"Optimizer: \" + str(optimizer) + '\\n',\n",
        "                                        \"learning_rate:\"+ str(self.lr)+ '\\n',\n",
        "                                        \"Number layers: \" + str(self.number_layers) + '\\n',\n",
        "                                         \"batch_val_size: \" + str(self.val_batch_size) + '\\n',\n",
        "                                        \"batch_train_size: \" + str(self.train_batch_size) + '\\n',\n",
        "                                       ])\n",
        "            file_object.close()\n",
        "\n",
        "        self.CreateLoader(need_names=need_names)\n",
        "\n",
        "        loss_epochs = []\n",
        "        val_loss_epochs = []\n",
        "\n",
        "        for epoch in range(self.number_epochs):\n",
        "            # print(\"epoc is\", epoc)\n",
        "            print(\"epoch is\", epoch)\n",
        "            if (epoch + epoc) == 0 :\n",
        "                self.model.eval()\n",
        "            if (epoch + epoc) > 0 :\n",
        "                self.model.train()\n",
        "            running_loss = 0.0\n",
        "            for i, minibatch in enumerate(self.train_loader, 0):\n",
        "                #print(i)\n",
        "                if need_names == \"yes\":\n",
        "                    [name, x_true, x_degraded] = minibatch\n",
        "\n",
        "                if need_names == \"no\":\n",
        "                    [x_true, x_degraded] = minibatch\n",
        "                #sel = random.permutation(d)\n",
        "                # sel = sel[0 : self.train_batch_size]   # indices of the nonzero elements of xsharp\n",
        "                # for iter in range(self.train_batch_size):\n",
        "\n",
        "\n",
        "                x_true = x_true.type(self.dtype)\n",
        "                x_degraded = x_degraded.type(self.dtype)\n",
        "                # if epoch == 0:\n",
        "                #     x_true_cpu0 = x_true[0, :, :].cpu()\n",
        "                #     x_true_cpu0 = np.reshape(x_true_cpu0, [28, 28])\n",
        "                #     x_degraded_cpu0 = x_degraded[0, :, :].cpu()\n",
        "                #     x_degraded_cpu0 = np.reshape(x_degraded_cpu0, [28, 28])\n",
        "                #     #x_pred_cpu = x_pred.cpu().detach().numpy()\n",
        "                #     #x_pred_cpu = np.reshape(x_pred_cpu, [28, 28])\n",
        "                #     fig, axs = plt.subplots(1, 2, figsize=(10, 5))\n",
        "                #     axs[0].imshow(x_true_cpu0, cmap='gray',vmin=0, vmax=1)\n",
        "                #     axs[0].set_title('Original')\n",
        "                #     axs[1].imshow(x_degraded_cpu0, cmap='gray',vmin=0, vmax=1)\n",
        "                #     axs[1].set_title('Blurred, noisy')\n",
        "                # print(\"loss is\", Loss_fun(x_true, x_degraded).detach())\n",
        "                x0 = torch.zeros((self.train_batch_size, x_true.size()[1] * x_true.size()[2])).type(self.dtype)\n",
        "                x0_d = torch.zeros((self.train_batch_size, x_degraded.size()[1] * x_degraded.size()[2])).type(self.dtype)\n",
        "                if (epoch + epoc) == 0:\n",
        "                    p0 = x0\n",
        "                    p0_old = x0\n",
        "                    z0 = x0\n",
        "                    z0_old = x0\n",
        "                    d0 = x0_d\n",
        "                    d0_old = x0_d\n",
        "                    y = x_degraded\n",
        "                    y_cpu = y.cpu()\n",
        "\n",
        "                    y = torch.reshape(y_cpu, [y_cpu.size()[0], y_cpu.size()[1] * y_cpu.size()[2]])\n",
        "                    #print(y.size())\n",
        "                    x_pred, x_pred1 = self.model(self.H, p0, p0_old, d0, d0_old, z0, z0_old, y, x_true)\n",
        "                    #print(x_pred.size())\n",
        "                    x_true_cpu = x_true.cpu()\n",
        "                    x_true = torch.reshape(x_true_cpu, [x_true_cpu.size()[0], x_true_cpu.size()[1] * x_true_cpu.size()[2]])\n",
        "                    #print(x_true.size())\n",
        "                    x_pred = x_pred.to(device)\n",
        "                    x_true = x_true.to(device)\n",
        "                    loss = Loss_fun(x_pred, x_true).detach()\n",
        "                    running_loss += loss.item()\n",
        "\n",
        "                if (epoch + epoc) > 0:\n",
        "                    optimizer.zero_grad()\n",
        "                    p0 = x0\n",
        "                    p0_old = x0\n",
        "                    z0 = x0\n",
        "                    z0_old = x0\n",
        "                    d0 = x0_d\n",
        "                    d0_old = x0_d\n",
        "                    y = x_degraded\n",
        "                    y_cpu = y.cpu()\n",
        "                    y = torch.reshape(y_cpu, [y_cpu.size()[0], y_cpu.size()[1] * y_cpu.size()[2]])\n",
        "                    x_pred, x_pred1 = self.model(self.H, p0, p0_old, d0, d0_old, z0, z0_old, y, x_true)\n",
        "                    x_true_cpu = x_true.cpu()\n",
        "                    x_true = torch.reshape(x_true_cpu, [x_true_cpu.size()[0], x_true_cpu.size()[1] * x_true_cpu.size()[2]])\n",
        "                    x_pred = x_pred.to(device)\n",
        "                    x_true = x_true.to(device)\n",
        "                    # print(Loss_fun(x_true.cpu(), y.cpu()).detach())\n",
        "                    loss = Loss_fun(x_pred, x_true)\n",
        "                    loss.backward()\n",
        "                    optimizer.step()\n",
        "                    running_loss += loss.item()\n",
        "                    torch.autograd.set_detect_anomaly(True)\n",
        "            loss_epochs.append(running_loss / (len(self.train_loader)))\n",
        "            print(\"train loss for epoch\", epoch + epoc , \"is\", running_loss / (len(self.train_loader)))\n",
        "\n",
        "            # saving model here\n",
        "            torch.save({\n",
        "                    'epoch': epoch + epoc ,\n",
        "                    'model_state_dict': self.model.state_dict(),\n",
        "                    'optimizer_state_dict': optimizer.state_dict(),\n",
        "                    'loss': loss, },\n",
        "                    os.path.join(self.path_save_model,'Trained_Model'+'_'+str(number_try))+ '/epoch'+ str(epoch + epoc ))\n",
        "\n",
        "            #Evaluation on validation set\n",
        "            self.model.eval()\n",
        "            with torch.no_grad():\n",
        "                loss_current_val = 0\n",
        "                for minibatch in self.val_loader:\n",
        "                    if need_names == \"yes\":\n",
        "                        [name, x_true, x_degraded] = minibatch\n",
        "                    if need_names == \"no\":\n",
        "                        [x_true, x_degraded] = minibatch\n",
        "\n",
        "                    x_true = x_true.type(self.dtype)\n",
        "                    x_degraded = x_degraded.type(self.dtype)\n",
        "                    x0 = torch.zeros((self.train_batch_size, x_true.size()[1] * x_true.size()[2])).type(self.dtype)\n",
        "                    x0_d = torch.zeros((self.train_batch_size, x_degraded.size()[1] * x_degraded.size()[2])).type(self.dtype)\n",
        "\n",
        "                    p0 = x0\n",
        "                    p0_old = x0\n",
        "                    z0 = x0\n",
        "                    z0_old = x0\n",
        "                    d0 = x0_d\n",
        "                    d0_old = x0_d\n",
        "                    y = x_degraded\n",
        "                    y_cpu = y.cpu()\n",
        "                    y = torch.reshape(y_cpu, [y_cpu.size()[0], y_cpu.size()[1] * y_cpu.size()[2]])\n",
        "                    x_pred, x_pred1 =self.model(self.H,p0,p0_old,d0,d0_old,z0,z0_old,y,x_true)\n",
        "                    x_true_cpu = x_true.cpu()\n",
        "                    x_true = torch.reshape(x_true_cpu, [x_true_cpu.size()[0], x_true_cpu.size()[1] * x_true_cpu.size()[2]])\n",
        "                    x_pred = x_pred.to(device)\n",
        "                    x_true = x_true.to(device)\n",
        "                    loss_val = Loss_fun(x_pred, x_true)\n",
        "                    loss_current_val += torch.Tensor.item(loss_val)\n",
        "                val_loss_epochs.append(loss_current_val / (len(self.val_loader)))\n",
        "                print(\"val loss for epoch\", epoch + epoc , \"is\", loss_current_val / (len(self.val_loader)))\n",
        "                #Saving training evolution in readme file\n",
        "                file_object = open(\n",
        "                    os.path.join(self.path_save_model, 'Trained_Model'  + '_' + str(number_try)) + \"/readme.txt\",\n",
        "                        \"a\")\n",
        "                file_object.writelines([ \"Train loss for epoch \"+ str(epoch + epoc)+ \"is: \"+ str( running_loss / (len(self.train_loader)))+'\\n',\n",
        "                                            \"Val loss for epoch\" +str(epoch + epoc ) +\"is: \"+ str(loss_current_val / (len(self.val_loader)))+ '\\n'\n",
        "                                           ])\n",
        "                file_object.close()\n",
        "                plt.plot(loss_epochs,color='black', linestyle='dashed', linewidth=1)\n",
        "                plt.plot(val_loss_epochs,color='blue' ,linestyle='dashed', linewidth=1)\n",
        "                axes = plt.gca()\n",
        "                axes.set_xlabel('Epochs')\n",
        "                axes.set_ylabel('Average MSE loss')\n",
        "                plt.legend([\"Training loss\", \"Validation loss\"])\n",
        "                plt.savefig(os.path.join(self.path_save_model, 'Trained_Model'  + '_' + str(number_try))+'/Trained_Model'  + str(number_try)+\"Training_curves.png\")\n",
        "        return\n",
        "\n",
        "    def test(self, path_set=None, path_model=None, need_names=\"no\", path_signal=None, path_save_estimate=False):\n",
        "        checkpoint = torch.load(path_model, map_location='cuda:0')\n",
        "        self.model.load_state_dict(checkpoint['model_state_dict'])\n",
        "        self.model.eval()\n",
        "\n",
        "        if path_signal is not None :\n",
        "\n",
        "            name = os.path.split(path_signal)[1]\n",
        "            x_true = torch.unsqueeze(torch.tensor(np.load(path_signal, allow_pickle=True)),0)\n",
        "            x_degraded = torch.unsqueeze(torch.tensor(np.load(path_signal.replace('Groundtruth', 'Degraded').replace('_Gr_','_Dr_'), allow_pickle=True)), 0)\n",
        "            x_true = x_true.type(self.dtype)\n",
        "            x_degraded = x_degraded.type(self.dtype)\n",
        "            x_0 = torch.zeros((1, x_true.size()[1] * x_true.size()[2])).type(self.dtype)\n",
        "            x0_d = torch.zeros((1, x_true.size()[1] * x_true.size()[2])).type(self.dtype)\n",
        "\n",
        "            t0 = time.time()\n",
        "            p0 = x_0\n",
        "            p0_old = x_0\n",
        "            z0 = x_0\n",
        "            z0_old = x_0\n",
        "            d0 = x0_d\n",
        "            d0_old = x0_d\n",
        "            y = x_degraded\n",
        "            y_cpu = y.cpu()\n",
        "            y = torch.reshape(y_cpu, [y_cpu.size()[0], y_cpu.size()[1] * y_cpu.size()[2]])\n",
        "            x_pred, x_pred1 =self.model(self.H, p0, p0_old, d0, d0_old, z0, z0_old, y, x_true)\n",
        "            t1 = time.time()\n",
        "            x_true_cpu = x_true.cpu()\n",
        "            x_true = torch.reshape(x_true_cpu, [x_true_cpu.size()[0], x_true_cpu.size()[1] * x_true_cpu.size()[2]])\n",
        "            loss = Loss_fun(x_pred, x_true).detach()\n",
        "            print(\"loss is:\", loss)\n",
        "            print(\"Execution time is\",t1 - t0)\n",
        "            return x_pred.detach()\n",
        "        else:\n",
        "            self.CreateLoader(need_names)\n",
        "            total_loss = 0\n",
        "            total_time = 0\n",
        "            high_MSE = 0\n",
        "            i = 0\n",
        "            MSE_list = []\n",
        "\n",
        "            for minibatch in self.test_loader:\n",
        "                if need_names == \"yes\":\n",
        "                    [name, x_true, x_degraded] = minibatch\n",
        "                    name = name[0]\n",
        "                if need_names == \"no\":\n",
        "                    [x_true, x_degraded] = minibatch\n",
        "\n",
        "                x_true = x_true.type(self.dtype)\n",
        "\n",
        "                #print(\"shape x_true_else\", x_true.size())\n",
        "                x_degraded = x_degraded.type(self.dtype)\n",
        "\n",
        "                x0_d = torch.zeros((self.test_batch_size, x_true.size()[1] * x_true.size()[2])).type(self.dtype)\n",
        "                x_0 = torch.zeros((self.test_batch_size, x_true.size()[1] * x_true.size()[2])).type(self.dtype)\n",
        "                t0 = time.time()\n",
        "                p0 = x_0\n",
        "                p0_old = x_0\n",
        "                z0 = x_0\n",
        "                z0_old = x_0\n",
        "                d0 = x0_d\n",
        "                d0_old = x0_d\n",
        "                y = x_degraded\n",
        "                y_cpu = y.cpu()\n",
        "                y = torch.reshape(y_cpu, [y_cpu.size()[0], y_cpu.size()[1] * y_cpu.size()[2]])\n",
        "                #print(\"size y_else\", y.size())\n",
        "                x_pred, x_pred1 = self.model(self.H, p0, p0_old, d0, d0_old, z0, z0_old, y, x_true) #Primal and dual output of ChPoVe\n",
        "                #print(\"shape x_pred_else\", x_pred.size())\n",
        "\n",
        "                t1 = time.time()\n",
        "                x_true = torch.reshape(x_true, [x_true.size()[0], x_true.size()[1] * x_true.size()[2]])\n",
        "                #print(\"shape x_true_else_reshape\", x_true.size())\n",
        "                if i<=10:\n",
        "                    x_true_cpu = x_true.cpu()\n",
        "                    x_true_cpu = np.reshape(x_true_cpu, [28, 28])\n",
        "                    y_cpu = y.cpu()\n",
        "                    y_cpu = np.reshape(y_cpu, [28, 28])\n",
        "                    x_pred_cpu = x_pred.cpu().detach().numpy()\n",
        "                    x_pred_cpu = np.reshape(x_pred_cpu, [28, 28])\n",
        "                    fig, axs = plt.subplots(1, 3, figsize=(10, 5))\n",
        "                    axs[0].imshow(x_true_cpu, cmap='gray',vmin=0, vmax=1)\n",
        "                    axs[0].set_title('Original')\n",
        "                    axs[1].imshow(y_cpu, cmap='gray',vmin=0, vmax=1)\n",
        "                    axs[1].set_title('Blurred, noisy')\n",
        "                    axs[2].imshow(x_pred_cpu, cmap='gray',vmin=0, vmax=1)\n",
        "                    axs[2].set_title('Unrolling')\n",
        "                loss = Loss_fun(x_true, x_pred).detach()\n",
        "                MSE_list.append(loss)\n",
        "\n",
        "                total_loss += loss\n",
        "                total_time += t1-t0\n",
        "                i+=1\n",
        "\n",
        "            #compute metrics STD\n",
        "            mse_std = 0\n",
        "            for l in MSE_list:\n",
        "                mse_std = mse_std + ((l - total_loss / len(self.test_loader)) ** 2)\n",
        "            mse_std = torch.sqrt(mse_std/(len(self.test_loader)-1))\n",
        "\n",
        "            print(\"Average MSE loss is \", total_loss / len(self.test_loader), \"Average execution time is \", total_time / len(self.test_loader))\n",
        "            print(\"the standard deviation of MSE loss is\", float(mse_std))\n",
        "\n",
        "            return"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Gm9px8D2_-jw"
      },
      "outputs": [],
      "source": [
        "# Set dataset paths directly\n",
        "path_train = \"/content/Dataset1/Train/\"\n",
        "path_validation = \"/content/Dataset1/Validation/\"\n",
        "path_test = \"/content/Dataset1/Test/\"\n",
        "path_save_model = \"/content/Models/\"\n",
        "paths = [path_train, path_validation, path_test, path_save_model]\n",
        "\n",
        "\n",
        "N = 28\n",
        "# Artificial blur\n",
        "sigma = 1 #too much blur?\n",
        "\n",
        "# Since N is very small, we can implement the blurring operator as a matrix\n",
        "K_Mat = np.eye(N ** 2)\n",
        "for i in range(N ** 2):\n",
        "    figu = K_Mat[:, i].reshape(N, N)\n",
        "    K_Mat[:, i] = scipy.ndimage.gaussian_filter(figu, sigma).reshape(N ** 2)\n",
        "KtK_Mat = np.matmul(np.transpose(K_Mat), K_Mat)\n",
        "\n",
        "# We need to use K_Mat in pytorch (possibly on CUDA)\n",
        "K_Mat_torch = torch.from_numpy(K_Mat).type(torch.cuda.DoubleTensor if device == \"cuda\" else torch.DoubleTensor).to(device)\n",
        "H = K_Mat_torch\n",
        "\n",
        "####################Network Initilaization##############\n",
        "num_layers = 25\n",
        "initial_x0 = \"Null_initialization\"\n",
        "Initialization = [num_layers, H, initial_x0]\n",
        "########################################################\n",
        "\n",
        "##################Train conditions######################\n",
        "number_epochs = 100\n",
        "learning_rate = 1e-2#before 0.0001\n",
        "train_batch_size = 5\n",
        "val_batch_size = 5\n",
        "test_batch_size = 1 #mouna sets it to 1\n",
        "train_conditions = [number_epochs, learning_rate, train_batch_size, val_batch_size, test_batch_size]\n",
        "\n",
        "Net = Network(Initialization, train_conditions, paths)\n",
        "\n",
        "# To train uncomment next line\n",
        "Net.train(number_try=1, need_names='yes')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "rENLOC7n9iF3"
      },
      "outputs": [],
      "source": [
        "#To test uncomment next lines\n",
        "Net = Network(Initialization, train_conditions, paths)\n",
        "Net.test(path_set=path_test, path_model=\"Models/Trained_Model_1/epoch99\", need_names='yes')"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3 (ipykernel)",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.13"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}